{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iticbcn/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "pc= Pinecone(api_key=\"pcsk_4KEUVc_LLTgHAGuEJWZqwmzifDdkdUybpsNfUoSYmaZGWwfHHzhtE4GsqmxfqcF47vWeRk\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "index_name = \"kibalion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_size):\n",
    "        self.context_size=context_size\n",
    "        tokens= tokenizer.encode(txt)\n",
    "        # self.sentencedb = [tokens[i:i+self.context_size] for i in range(0, len(tokens), self.context_size)]\n",
    "        self.sentencedb = [tokenizer.decode(tokens[i:i+self.context_size]) for i in range(0, len(tokens), self.context_size)]\n",
    "        \n",
    "                \n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.sentencedb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentencedb[idx-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68177 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "with open(\"Kibalion.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        txt=file.read() \n",
    "    \n",
    "dataset = dataset(txt, tokenizer, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_index = pc.create_index(\n",
    "\n",
    "    name=index_name,\n",
    "\n",
    "    dimension=384, # Replace with your model dimensions\n",
    "\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "\n",
    "    spec=ServerlessSpec(\n",
    "\n",
    "        cloud=\"aws\",\n",
    "\n",
    "        region=\"us-east-1\"\n",
    "\n",
    "    ) \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39mIndex(index_name)\n\u001b[0;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Subir los datos a Pinecone\u001b[39;00m\n",
      "File \u001b[0;32m~/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:586\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    585\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 586\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_length(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[1;32m    587\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [sentences[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "embeddings = model.encode(dataset)\n",
    "\n",
    "print(embeddings.shape)\n",
    "\n",
    "# Subir los datos a Pinecone\n",
    "for i, embed in enumerate(embeddings):\n",
    "    index.upsert(vectors=[(str(i), embed.tolist(), {\"text\": dataset[i]})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos recuperados: ['considera la influencia que el pensamiento hermetico ejerciera sobre los filosofos primitivos de grecia, sobre cuyas doctrinas descansan en gran parte las teorias de la cien - cia actual. la aceptacion del primer principio hermetico ( mentalismo ) es la unica gran diferencia entre la ciencia moderna y los estudi', '##estros dias nos es dable encontrar algunos libros valiosos de filosofia hermetica, pero la mayor parte se ha perdido. sin em - bargo, la filosofia hermetica es la unica clave maestra que puede abrir las puertas a todas las ensenanzas ocultas. en los primeros tiempos existio una compila', '##ipio hermetico que se oculta tras todas esas varias formas de operar, buenas o malas, porque la fuerza puede ser empleada en ambas direcciones, de acuerdo con el principio hermetico de polaridad. en esta obrita indicaremos los principios basicos en los que se funda la transmutacion mental, de tal manera que tod', '. » el kybalion el tercer gran principio hermetico — el principio de la vibracion — encierra la verdad de que el movimiento se manifiesta en todo el universo. nada esta en reposo, todo se mueve vibra y circula. este principio her - metico fue reconocido por algunos de los primitivos fi', '##anzas hermeticas concernientes al genero mental. los instructores hermeticos imparten ensenanzas concernientes a este punto, pidiendo a sus discipulos que se atengan al proceso de su propia con - ciencia, a su propio yo. el discipulo fija entonces su atencion internamente sobre el ego que esta en cada uno de nosot']\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "\n",
    "query = \"Explicame un principio hermetico\"\n",
    "\n",
    "query_embedding = model.encode([query]).tolist()\n",
    "\n",
    "# Buscar en Pinecone\n",
    "results = index.query(vector=query_embedding[0], top_k=5, include_metadata=True)\n",
    "\n",
    "retrieved_texts = [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
    "print(\"Documentos recuperados:\", retrieved_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]\n",
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at projecte-aina/FlorRAG and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "/home/iticbcn/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "# argument needs to be of type (SquadExample, dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_response(answer)\n\u001b[0;32m---> 36\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgivePrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mgivePrediction\u001b[0;34m(instruction, context, max_new_tokens, repetition_penalty, top_k, top_p, do_sample, temperature, eos_token_id)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgivePrediction\u001b[39m(instruction, context, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Instruction\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Context\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Answer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m     answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[0;32m~/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:396\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    391\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:227\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[0;32m--> 227\u001b[0m     inputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/Escritorio/CE/IA/Virtualsenv/pyTorch/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:172\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.normalize\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuestionAnsweringPipeline\u001b[38;5;241m.\u001b[39mcreate_sample(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mitem)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m argument needs to be of type (SquadExample, dict)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: # argument needs to be of type (SquadExample, dict)"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"projecte-aina/FlorRAG\")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"projecte-aina/FlorRAG\", device=-1)\n",
    "\n",
    "\n",
    "context = retrieved_texts\n",
    "\n",
    "\n",
    "def clean_response(response):\n",
    "    # Encuentra la última aparición de un punto, exclamación o interrogación\n",
    "    match = re.search(r\".*[.!?]\", response)  # Busca la última ocurrencia de ., ! o ?\n",
    "    if match:\n",
    "        return match.group(0).strip()  # Devuelve el texto hasta ese punto\n",
    "    return response.strip()  # Si no encuentra, devuelve el texto completo\n",
    "\n",
    "\n",
    "def givePrediction(instruction, context, max_new_tokens=200, repetition_penalty=1.2, top_k=50, top_p=0.8, do_sample=True, temperature=0.2, eos_token_id=None):\n",
    "    text = f\"### Instruction\\n{{instruction}}\\n### Context\\n{{context}}\\n### Answer\\n\"\n",
    "    response = pipe(text.format(instruction=instruction, context=context),\n",
    "                    temperature=temperature,\n",
    "                    repetition_penalty=repetition_penalty, \n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    top_k=top_k, \n",
    "                    top_p=top_p, \n",
    "                    do_sample=do_sample,  \n",
    "                    eos_token_id=eos_token_id)[0][\"generated_text\"]\n",
    "    \n",
    "    answer = response.split(\"###\")[-1][8:-1]\n",
    "    print(answer)\n",
    "    return clean_response(answer)\n",
    "\n",
    "answer = givePrediction(query, context, eos_token_id=eos_token_id)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
